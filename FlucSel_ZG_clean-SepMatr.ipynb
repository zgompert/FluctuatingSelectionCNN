{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7646f847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /scratch/local/u6031121/10746/matplotlib-mxycmpcn because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "### loading libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf57219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Device name: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if len(devices) > 0:\n",
    "    print(\"GPU is available\")\n",
    "    for device in devices:\n",
    "        print(\"Device name:\", device.name)\n",
    "else:\n",
    "    print(\"No GPU is available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be167a8b",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e94e3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################ TRAINING DATA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dcd447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 10, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "########### Training Data\n",
    "#Set the directory paths for each type of CSV file\n",
    "directory_path_channel1 = '/uufs/chpc.utah.edu/common/home/gompert-group4/projects/fluctCNN/DataSepFilesNov23/EnvFiles/trainEnv/'\n",
    "directory_path_channel2 = '/uufs/chpc.utah.edu/common/home/gompert-group4/projects/fluctCNN/DataSepFilesNov23/FreqFiles/trainFreq/'\n",
    "\n",
    "# Get lists of all CSV files in each directory\n",
    "csv_files_channel1 = [file for file in os.listdir(directory_path_channel1) if file.endswith('.csv')]\n",
    "csv_files_channel2 = [file for file in os.listdir(directory_path_channel2) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize lists to store matrices and labels\n",
    "# Process each pair of CSV files\n",
    "matrices_channel1 = []\n",
    "matrices_channel2 = []\n",
    "out_vec1 = []\n",
    "\n",
    "for file_channel1, file_channel2 in zip(csv_files_channel1, csv_files_channel2):\n",
    "    # Read CSV file for channel 1\n",
    "    df_channel1 = pd.read_csv(os.path.join(directory_path_channel1, file_channel1), header=None, sep=',', dtype='float32')\n",
    "    matrices_channel1.append(df_channel1)\n",
    "    \n",
    "    # Read CSV file for channel 2\n",
    "    df_channel2 = pd.read_csv(os.path.join(directory_path_channel2, file_channel2), header=None, sep=',', dtype='float32')\n",
    "    matrices_channel2.append(df_channel2)\n",
    "    \n",
    "    # Appending first eight letters to out_vec as labels (adjust as needed)\n",
    "    label = file_channel2[:9]\n",
    "    out_vec1.append(label)\n",
    "# Stack matrices for each channel\n",
    "input_arrays_channel1 = np.stack(matrices_channel1)\n",
    "input_arrays_channel2 = np.stack(matrices_channel2)\n",
    "# Reshape the input arrays\n",
    "inDa_trn_channel1 = input_arrays_channel1.reshape((len(csv_files_channel1), 10, 10))\n",
    "inDa_trn_channel2 = input_arrays_channel2.reshape((len(csv_files_channel2), 10, 10))\n",
    "\n",
    "\n",
    "# Stack the channels\n",
    "inDa_trn = np.stack([inDa_trn_channel1, inDa_trn_channel2], axis=-1)\n",
    "\n",
    "# Print the shape of the final input array\n",
    "print(inDa_trn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb10ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### TRAINIG labels\n",
    "labels_train =out_vec1\n",
    "# converting string values to integer calsses\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit the LabelEncoder to your string list and transform the string values into integer classes\n",
    "label_train_int = label_encoder.fit_transform(labels_train)\n",
    "#from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "ouLa_trn =to_categorical(label_train_int)\n",
    "ouLa_trn.shape\n",
    "#print(ouLa_trn)\n",
    "ouLa_trn.shape\n",
    "labelint_train = label_train_int\n",
    "labelint_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b478579",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02db96b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "############################ TEST DATA \n",
    "# changing directory to the data folder\n",
    "########### Training Data\n",
    "#Set the directory paths for each type of CSV file\n",
    "directory_path_tst_channel1 = '/uufs/chpc.utah.edu/common/home/gompert-group4/projects/fluctCNN/DataSepFilesNov23/EnvFiles/testEnv/'\n",
    "directory_path_tst_channel2 = '/uufs/chpc.utah.edu/common/home/gompert-group4/projects/fluctCNN/DataSepFilesNov23/FreqFiles/testFreq/'\n",
    "\n",
    "# Get lists of all CSV files in each directory\n",
    "csv_files_tst_channel1 = [file for file in os.listdir(directory_path_tst_channel1) if file.endswith('.csv')]\n",
    "csv_files_tst_channel2 = [file for file in os.listdir(directory_path_tst_channel2) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize lists to store matrices and labels\n",
    "# Process each pair of CSV files\n",
    "matrices_tst_channel1 = []\n",
    "matrices_tst_channel2 = []\n",
    "out_vec2 = []\n",
    "\n",
    "for file_channel1, file_channel2 in zip(csv_files_tst_channel1, csv_files_tst_channel2):\n",
    "    # Read CSV file for channel 1\n",
    "    df_tst_channel1 = pd.read_csv(os.path.join(directory_path_tst_channel1, file_channel1), header=None, sep=',', dtype='float32')\n",
    "    matrices_tst_channel1.append(df_tst_channel1)\n",
    "    \n",
    "    # Read CSV file for channel 2\n",
    "    df_tst_channel2 = pd.read_csv(os.path.join(directory_path_tst_channel2, file_channel2), header=None, sep=',', dtype='float32')\n",
    "    matrices_tst_channel2.append(df_tst_channel2)\n",
    "    \n",
    "    # Appending first eight letters to out_vec as labels (adjust as needed)\n",
    "    label = file_channel2[:9]\n",
    "    out_vec2.append(label)\n",
    "# Stack matrices for each channel\n",
    "input_arrays_tst_channel1 = np.stack(matrices_tst_channel1)\n",
    "input_arrays_tst_channel2 = np.stack(matrices_tst_channel2)\n",
    "# Reshape the input arrays\n",
    "inDa_tst_channel1 = input_arrays_tst_channel1.reshape((len(csv_files_tst_channel1), 10, 10))\n",
    "inDa_tst_channel2 = input_arrays_tst_channel2.reshape((len(csv_files_tst_channel2), 10, 10))\n",
    "\n",
    "\n",
    "# Stack the channels\n",
    "inDa_tst = np.stack([inDa_tst_channel1, inDa_tst_channel2], axis=-1)\n",
    "\n",
    "# Print the shape of the final input array\n",
    "print(inDa_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129028fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### TEST labels\n",
    "\n",
    "labels_tst =out_vec2\n",
    "# converting string values to integer calsses\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit the LabelEncoder to your string list and transform the string values into integer classes\n",
    "label_tst_int = label_encoder.fit_transform(labels_tst)\n",
    "#from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "ouLa_tst =to_categorical(label_tst_int)\n",
    "ouLa_tst.shape\n",
    "#print(ouLa_trn)\n",
    "ouLa_tst.shape\n",
    "labelint_tst = label_tst_int\n",
    "labelint_tst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d0a8f",
   "metadata": {},
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c5be3",
   "metadata": {},
   "source": [
    "#### Independent validation set generated afresh from the simulation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ffdba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "############################ VALIDATION DATA \n",
    "\n",
    "# changing directory to the data folder\n",
    "########### Training Data\n",
    "#Set the directory paths for each type of CSV file\n",
    "directory_path_val_channel1 = '/uufs/chpc.utah.edu/common/home/gompert-group4/projects/fluctCNN/DataSepFilesNov23/EnvFiles/valEnv/'\n",
    "directory_path_val_channel2 = '/uufs/chpc.utah.edu/common/home/gompert-group4/projects/fluctCNN/DataSepFilesNov23/FreqFiles/valFreq/'\n",
    "\n",
    "# Get lists of all CSV files in each directory\n",
    "csv_files_val_channel1 = [file for file in os.listdir(directory_path_val_channel1) if file.endswith('.csv')]\n",
    "csv_files_val_channel2 = [file for file in os.listdir(directory_path_val_channel2) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize lists to store matrices and labels\n",
    "# Process each pair of CSV files\n",
    "matrices_val_channel1 = []\n",
    "matrices_val_channel2 = []\n",
    "out_vec3 = []\n",
    "\n",
    "for file_channel1, file_channel2 in zip(csv_files_val_channel1, csv_files_val_channel2):\n",
    "    # Read CSV file for channel 1\n",
    "    df_val_channel1 = pd.read_csv(os.path.join(directory_path_val_channel1, file_channel1), header=None, sep=',', dtype='float32')\n",
    "    matrices_val_channel1.append(df_val_channel1)\n",
    "    \n",
    "    # Read CSV file for channel 2\n",
    "    df_val_channel2 = pd.read_csv(os.path.join(directory_path_val_channel2, file_channel2), header=None, sep=',', dtype='float32')\n",
    "    matrices_val_channel2.append(df_val_channel2)\n",
    "    \n",
    "    # Appending first eight letters to out_vec as labels (adjust as needed)\n",
    "    label = file_channel2[:9]\n",
    "    out_vec3.append(label)\n",
    "# Stack matrices for each channel\n",
    "input_arrays_val_channel1 = np.stack(matrices_val_channel1)\n",
    "input_arrays_val_channel2 = np.stack(matrices_val_channel2)\n",
    "# Reshape the input arrays\n",
    "inDa_val_channel1 = input_arrays_val_channel1.reshape((len(csv_files_val_channel1), 10, 10))\n",
    "inDa_val_channel2 = input_arrays_val_channel2.reshape((len(csv_files_val_channel2), 10, 10))\n",
    "\n",
    "\n",
    "# Stack the channels\n",
    "inDa_val = np.stack([inDa_val_channel1, inDa_val_channel2], axis=-1)\n",
    "\n",
    "# Print the shape of the final input array\n",
    "print(inDa_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a980f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_val =out_vec3\n",
    "# converting string values to integer calsses\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit the LabelEncoder to your string list and transform the string values into integer classes\n",
    "label_val_int = label_encoder.fit_transform(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1a87f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### VALIDATION labels\n",
    "labels_val =out_vec3\n",
    "# converting string values to integer calsses\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit the LabelEncoder to your string list and transform the string values into integer classes\n",
    "label_val_int = label_encoder.fit_transform(labels_val)\n",
    "#from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "ouLa_val =to_categorical(label_val_int)\n",
    "ouLa_val.shape\n",
    "#print(ouLa_trn)\n",
    "ouLa_val.shape\n",
    "labelint_val = label_val_int\n",
    "labelint_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5dd81",
   "metadata": {},
   "source": [
    "# # Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba727d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca54501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshaping arrays for randomforest\n",
    "X_flattened_train = inDa_trn.reshape((inDa_trn.shape[0], -1))\n",
    "X_flattened_tst = inDa_tst.reshape((inDa_tst.shape[0], -1))\n",
    "X_flattened_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92590f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(min_samples_split=4, n_estimators=200, random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Create and train the Random Forest model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=42, min_samples_split=4)\n",
    "rf_classifier.fit(X_flattened_train, labelint_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c220829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_classifier.predict(X_flattened_tst)\n",
    "accuracy = accuracy_score(labelint_tst, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ddd40c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.638\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94      1000\n",
      "           1       0.48      0.49      0.49      1000\n",
      "           2       0.49      0.52      0.51      1000\n",
      "\n",
      "    accuracy                           0.64      3000\n",
      "   macro avg       0.65      0.64      0.64      3000\n",
      "weighted avg       0.65      0.64      0.64      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(labelint_tst, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44b2dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[896  56  48]\n",
      " [ 13 495 492]\n",
      " [  4 473 523]]\n"
     ]
    }
   ],
   "source": [
    "# Additionally, you can visualize the confusion matrix\n",
    "conf_matrix = metrics.confusion_matrix(labelint_tst, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0773295",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43b7ece0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.2-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m142.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from xgboost) (1.8.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37989cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d7bbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b7c8ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_flattened_train, label=labelint_train)\n",
    "dtest = xgb.DMatrix(X_flattened_tst, label=labelint_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c39e9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters and train the model\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Change to 'multi:softmax' for multiclass classification\n",
    "    'eval_metric': 'logloss',         # Change based on your problem\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'num_class':3\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82575838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uufs/chpc.utah.edu/common/home/u6031121/.local/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [14:37:55] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "num_round = 100  # Number of boosting rounds\n",
    "\n",
    "model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6864d3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5523333333333333\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79      1000\n",
      "           1       0.43      0.91      0.58      1000\n",
      "           2       0.00      0.00      0.00      1000\n",
      "\n",
      "    accuracy                           0.55      3000\n",
      "   macro avg       0.42      0.55      0.46      3000\n",
      "weighted avg       0.42      0.55      0.46      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "accuracy = accuracy_score(labelint_tst, y_pred_binary)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_report(labelint_tst, y_pred_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27151bdc",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d41342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 16:00:13.423046: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 16:00:17.778953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9641 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1c:00.0, compute capability: 7.5\n",
      "2023-12-18 16:00:18.207434: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 16:00:22.052007: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2023-12-18 16:00:26.086762: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-18 16:00:26.089059: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-18 16:00:26.089081: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version\n",
      "2023-12-18 16:00:26.090437: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-18 16:00:26.090493: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 18s 2ms/step - loss: 1.1000 - accuracy: 0.3286 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0991 - accuracy: 0.3345 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0990 - accuracy: 0.3319 - val_loss: 1.0987 - val_accuracy: 0.3340\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0990 - accuracy: 0.3357 - val_loss: 1.0985 - val_accuracy: 0.3377\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0984 - accuracy: 0.3389 - val_loss: 1.0988 - val_accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0981 - accuracy: 0.3424 - val_loss: 1.0987 - val_accuracy: 0.3313\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0974 - accuracy: 0.3530 - val_loss: 1.0984 - val_accuracy: 0.3400\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0964 - accuracy: 0.3625 - val_loss: 1.0981 - val_accuracy: 0.3410\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0947 - accuracy: 0.3701 - val_loss: 1.0981 - val_accuracy: 0.3387\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0929 - accuracy: 0.3737 - val_loss: 1.0989 - val_accuracy: 0.3420\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0904 - accuracy: 0.3901 - val_loss: 1.0989 - val_accuracy: 0.3403\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0864 - accuracy: 0.3959 - val_loss: 1.0998 - val_accuracy: 0.3413\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0831 - accuracy: 0.4016 - val_loss: 1.0992 - val_accuracy: 0.3433\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0784 - accuracy: 0.4073 - val_loss: 1.1007 - val_accuracy: 0.3493\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0742 - accuracy: 0.4139 - val_loss: 1.1028 - val_accuracy: 0.3443\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0680 - accuracy: 0.4299 - val_loss: 1.1042 - val_accuracy: 0.3460\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0622 - accuracy: 0.4353 - val_loss: 1.1046 - val_accuracy: 0.3507\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0567 - accuracy: 0.4462 - val_loss: 1.1049 - val_accuracy: 0.3487\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0446 - accuracy: 0.4570 - val_loss: 1.1048 - val_accuracy: 0.3623\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0389 - accuracy: 0.4626 - val_loss: 1.1103 - val_accuracy: 0.3633\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0287 - accuracy: 0.4790 - val_loss: 1.1035 - val_accuracy: 0.3693\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0146 - accuracy: 0.4923 - val_loss: 1.1126 - val_accuracy: 0.3637\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.0038 - accuracy: 0.4957 - val_loss: 1.1015 - val_accuracy: 0.3817\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9917 - accuracy: 0.5057 - val_loss: 1.1091 - val_accuracy: 0.3893\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9778 - accuracy: 0.5259 - val_loss: 1.0840 - val_accuracy: 0.4047\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9575 - accuracy: 0.5398 - val_loss: 1.0730 - val_accuracy: 0.4157\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9380 - accuracy: 0.5489 - val_loss: 1.0604 - val_accuracy: 0.4317\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.9110 - accuracy: 0.5699 - val_loss: 1.0306 - val_accuracy: 0.4540\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8843 - accuracy: 0.5841 - val_loss: 1.0466 - val_accuracy: 0.4410\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8512 - accuracy: 0.6049 - val_loss: 0.9880 - val_accuracy: 0.4843\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8118 - accuracy: 0.6209 - val_loss: 0.9390 - val_accuracy: 0.5150\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.7662 - accuracy: 0.6521 - val_loss: 0.8826 - val_accuracy: 0.5460\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.7179 - accuracy: 0.6733 - val_loss: 0.8266 - val_accuracy: 0.5673\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6780 - accuracy: 0.6870 - val_loss: 0.7628 - val_accuracy: 0.5897\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6288 - accuracy: 0.7136 - val_loss: 0.7360 - val_accuracy: 0.6087\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5880 - accuracy: 0.7292 - val_loss: 0.6656 - val_accuracy: 0.6343\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5542 - accuracy: 0.7407 - val_loss: 0.7818 - val_accuracy: 0.5857\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5238 - accuracy: 0.7529 - val_loss: 0.6084 - val_accuracy: 0.6457\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5016 - accuracy: 0.7644 - val_loss: 0.6074 - val_accuracy: 0.6513\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4829 - accuracy: 0.7705 - val_loss: 0.5833 - val_accuracy: 0.6523\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4718 - accuracy: 0.7761 - val_loss: 0.5802 - val_accuracy: 0.6553\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4615 - accuracy: 0.7781 - val_loss: 0.5750 - val_accuracy: 0.6553\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4454 - accuracy: 0.7863 - val_loss: 0.7106 - val_accuracy: 0.6213\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4399 - accuracy: 0.7926 - val_loss: 0.6456 - val_accuracy: 0.6367\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4304 - accuracy: 0.7958 - val_loss: 0.6044 - val_accuracy: 0.6497\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4240 - accuracy: 0.7977 - val_loss: 0.5891 - val_accuracy: 0.6543\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4080 - accuracy: 0.8116 - val_loss: 0.5847 - val_accuracy: 0.6573\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3993 - accuracy: 0.8167 - val_loss: 0.6365 - val_accuracy: 0.6453\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3911 - accuracy: 0.8179 - val_loss: 0.6052 - val_accuracy: 0.6523\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3898 - accuracy: 0.8210 - val_loss: 0.6451 - val_accuracy: 0.6423\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3741 - accuracy: 0.8323 - val_loss: 0.6314 - val_accuracy: 0.6527\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3714 - accuracy: 0.8286 - val_loss: 0.6079 - val_accuracy: 0.6593\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3601 - accuracy: 0.8389 - val_loss: 0.6718 - val_accuracy: 0.6453\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3553 - accuracy: 0.8410 - val_loss: 0.6620 - val_accuracy: 0.6513\n",
      "Epoch 55/100\n",
      " 949/1875 [==============>...............] - ETA: 1s - loss: 0.3504 - accuracy: 0.8463"
     ]
    }
   ],
   "source": [
    "### CNN\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv2D(64, kernel_size=(3, 3),strides=(1, 1),padding = \"same\", activation=\"relu\", input_shape=(10,10,2)))\n",
    "model.add(layers.Conv2D(128, kernel_size=(2, 2), strides=(1, 1), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(3, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00005,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.99,\n",
    "    #momentum=0.4,\n",
    "    epsilon=1e-07,)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "es = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1)\n",
    "history = model.fit(inDa_trn, ouLa_trn, batch_size=8, epochs=100,validation_data=(inDa_tst, ouLa_tst), callbacks=[es] )\n",
    "print(history.history.keys())\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(inDa_val, ouLa_val)\n",
    "# print(\"Test Loss:\", loss)\n",
    "# print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c9a3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making confusion matrix to find percentage of false positives an dfalse negatives\n",
    "# use of test data to make predictions based on the model fitted\n",
    "lable_predicted = model.predict(inDa_val)\n",
    "lable_predicted_categories = np.argmax(lable_predicted, axis=1)\n",
    "label_actual = np.argmax(ouLa_val, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e48db97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculaiton of confusion matrix\\\n",
    "cm = confusion_matrix(label_actual, lable_predicted_categories)\n",
    "print(cm)\n",
    "type(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b97b5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through numpy array to convert the values into percentage\n",
    "\n",
    "oneD_total = np.sum(cm, axis =1)\n",
    "cm_perc = cm/oneD_total[:,np.newaxis]\n",
    "cm_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7e238f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [0,1,2] # the vector defines three classes of selection types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bd2d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphical visualization of the confusion matrix in the form of heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm_perc,  annot=True, cmap ='Blues', xticklabels =  class_names, yticklabels = class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b8d49dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 15, 24])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_2d = np.array([[1, 2, 3],\n",
    "                     [4, 5, 6],\n",
    "                     [7, 8, 9]])\n",
    "\n",
    "# Calculate the sum of each 1D array (row)\n",
    "row_sums = np.sum(array_2d, axis=1)\n",
    "row_sums"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
